{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE 598 Course Demo 2: Optimization for LASSO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO is widely used in Machine Learning and Statistics for sparse regression.\n",
    "\n",
    "Given data $(x_1,y_1),\\ldots, (x_N,y_N)$, where $x_i\\in R^d, y_i\\in R$, $i=1,\\ldots,N$, the LASSO model aims to solve the optimization problem\n",
    "$$\\min_{w} F(w):=\\frac{1}{2}\\sum_{i=1}^N (x_i^Tw-y_i)^2+\\lambda\\|w\\|_1$$\n",
    "where $\\lambda\\geq 0$ is the regularization parameter. \n",
    "\n",
    "In this demo, we illustrate and compare some of the algorithms learned in this module (subgradient descent, Nesterov's smoothing, proximal gradient, and accelerated gradient methods to solve LASSO and investigate their empirical peformances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "dim = 30\n",
    "lamda = 1/np.sqrt(N);\n",
    "np.random.seed(50)\n",
    "w = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim))).T\n",
    "X = np.matrix(np.random.multivariate_normal([0.0]*dim, np.eye(dim), size = N))\n",
    "y = X*w\n",
    "\n",
    "L = (np.linalg.svd(X)[1][0])**2\n",
    "print(L)\n",
    "max_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization via CVXPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CVX is a Matlab-based modeling system for convex optimization problems. http://cvxr.com/cvx/\n",
    "\n",
    "CVXPY is a Python-embedded modeling language for convex optimization problems. \n",
    "http://www.cvxpy.org/en/latest/index.html \n",
    "\n",
    "The default solver is EOCS (Embedded Conic Solver). See detials here.  https://www.embotech.com/ECOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "w = cvx.Variable(dim)\n",
    "loss = cvx.sum_squares(X*w-y)/2 + lamda * cvx.norm(w,1)\n",
    "\n",
    "problem = cvx.Problem(cvx.Minimize(loss))\n",
    "problem.solve(verbose=True) \n",
    "opt = problem.value\n",
    "print('Optimal Objective function value is: {}'.format(opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Algoirthms for Nonsmooth Convex Minimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define the objective, subgradient oracle, proximal operator, and gradient for smoothed function (Huber)  \n",
    "def obj(w):\n",
    "    r = X*w-y;\n",
    "    return np.sum(np.multiply(r,r))/2 +  lamda * np.sum(np.abs(w))\n",
    "\n",
    "def subgrad(w):\n",
    "    return  X.T*(X*w-y) + lamda*np.sign(w) \n",
    "\n",
    "def f_grad(w):\n",
    "    return  X.T*(X*w-y) \n",
    "\n",
    "def soft_threshod(w,mu):\n",
    "    return np.multiply(np.sign(w), np.maximum(np.abs(w)- mu,0))  \n",
    "\n",
    "def smooth_grad(w, mu):\n",
    "    temp = np.multiply((np.abs(w)<=mu),w/mu) + np.multiply((np.abs(w)>mu),np.sign(w)) \n",
    "    return X.T*(X*w-y) + lamda * temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Subgradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Subgradient Descent\n",
    "w = np.matrix([0.0]*dim).T\n",
    "obj_SD = []\n",
    "gamma = 0.01\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w = w - gamma * subgrad(w)/np.sqrt(t+1)\n",
    "    \n",
    "    obj_SD.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective error vs. iteration (log scale)\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "plt.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Nesterov's Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nesterov's Smoothing\n",
    "w = np.matrix([0.0]*dim).T\n",
    "smoothness = 0.01\n",
    "obj_SM = []\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w = w - smooth_grad(w, smoothness)/(L + lamda/smoothness)\n",
    "    \n",
    "    obj_SM.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective error vs. iteration (log scale)\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "plt.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Proximal Gadient \n",
    "w = np.matrix([0.0]*dim).T\n",
    "obj_PG = []\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w = w - (1/L)* f_grad(w)\n",
    "    w= soft_threshod(w,lamda/L)\n",
    "    \n",
    "    obj_PG.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot objective error vs. iteration (log scale)\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "plt.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "plt.semilogy(t, np.array(obj_PG)-opt, 'b', linewidth = 2, label = 'Proximal Gradient')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proximal Gradient with Backtracking Line-Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Proximal Gadient with Line Search\n",
    "w = np.matrix([0.0]*dim).T\n",
    "obj_PG_LS = []\n",
    "LL=1\n",
    "gamma = 1/LL\n",
    "beta = 1.2\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w_prev = w\n",
    "    delta = 1\n",
    "    while (delta>1e-3):\n",
    "        gamma = 1/LL\n",
    "        w = w_prev - gamma * f_grad(w_prev)    \n",
    "        w = soft_threshod(w,lamda * gamma)\n",
    "        delta = obj(w) - obj_val - f_grad(w_prev).T*(w-w_prev)- (LL/2) * np.linalg.norm(w-w_prev)**2\n",
    "        LL = LL*beta\n",
    "    LL = LL/beta   \n",
    "    \n",
    "    obj_PG_LS.append(obj_val.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective error vs. iteration (log scale)\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "plt.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "plt.semilogy(t, np.array(obj_PG)-opt, 'b--', linewidth = 2, label = 'Proximal Gradient')\n",
    "plt.semilogy(t, np.array(obj_PG_LS)-opt, 'b', linewidth = 2, label = 'Proximal Gradient (backtracking)')\n",
    "plt.legend(prop={'size':12})\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Accelerated Proximal Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Nesterovs' Accelerated Proximal Gradient\n",
    "w = np.matrix([0.0]*dim).T\n",
    "v = w\n",
    "obj_APG = []\n",
    "gamma = 1/L\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w_prev = w\n",
    "    w = v - gamma * f_grad(v)\n",
    "    w = soft_threshod(w,lamda * gamma)\n",
    "    v = w + t/(t+3) * (w - w_prev)\n",
    "\n",
    "    obj_APG.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective vs. iteration\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "ax.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "ax.semilogy(t, np.array(obj_PG)-opt, 'b', linewidth = 2, label = 'Proximal Gradient')\n",
    "ax.semilogy(t, np.array(obj_APG)-opt, 'c', linewidth = 2, label = 'Accelerated Proximal Gradient')\n",
    "ax.legend(prop={'size':12})\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerated Proximal Gradient with Backtracking Line-Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Nesterovs' Accelerated Proximal Gradient with Backtracking\n",
    "w = np.matrix([0.0]*dim).T\n",
    "v = w\n",
    "obj_APG_LS = []\n",
    "L=1\n",
    "gamma = 1/L\n",
    "beta = 1.2\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w_prev = w\n",
    "    delta = 1\n",
    "    while (delta>1e-3):\n",
    "        gamma = 1/L\n",
    "        w = v - gamma * f_grad(v)    \n",
    "        w = soft_threshod(w,lamda * gamma)\n",
    "        delta = obj(w) - obj_val - f_grad(w_prev).T*(w-w_prev)- (L/2) * np.linalg.norm(w-w_prev)**2\n",
    "        L = L*beta\n",
    "    L = L/beta    \n",
    "    v = w + t/(t+3) * (w - w_prev)\n",
    "\n",
    "    obj_APG_LS.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective vs. iteration\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "ax.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "# ax.semilogy(t, np.array(obj_PG)-opt, 'b--', linewidth = 2, label = 'Proximal Gradient')\n",
    "ax.semilogy(t, np.array(obj_PG_LS)-opt, 'b', linewidth = 2, label = 'Proximal Gradient (backtracking)')\n",
    "# ax.semilogy(t, np.array(obj_APG)-opt, 'c--', linewidth = 2, label = 'Accelerated Proximal Gradient')\n",
    "ax.semilogy(t, np.array(obj_APG_LS)-opt, 'c', linewidth = 2, label = 'Accelerated Proximal Gradient (backtracking)')\n",
    "ax.legend(prop={'size':12})\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Douglas Rachford Splitting/ADMM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ADMM\n",
    "w = np.matrix([0.0]*dim).T\n",
    "z = w\n",
    "u = w\n",
    "obj_ADMM = []\n",
    "rho = 5\n",
    "for t in range(0, max_iter):\n",
    "    obj_val = obj(w)\n",
    "    w = np.linalg.inv((X.T)*X + rho*np.identity(dim))*(X.T*y + rho*z - u )\n",
    "    z= soft_threshod(w + u/rho,lamda/rho)\n",
    "    u = u + rho * (w-z)\n",
    "    obj_ADMM.append(obj_val.item())\n",
    "    if (t%5==0):\n",
    "        print('iter= {},\\tobjective= {:3f}'.format(t, obj_val.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot objective vs. iteration\n",
    "t = np.arange(0, max_iter)\n",
    "fig, ax = plt.subplots(figsize = (9, 6))\n",
    "ax.semilogy(t, np.array(obj_SD)-opt, 'g', linewidth = 2, label = 'Subgradient Descent')\n",
    "plt.semilogy(t, np.array(obj_SM)-opt, 'm', linewidth = 2, label = 'Nesterov Smoothing')\n",
    "ax.semilogy(t, np.array(obj_PG)-opt, 'b', linewidth = 2, label = 'Proximal Gradient')\n",
    "ax.semilogy(t, np.array(obj_APG)-opt, 'c', linewidth = 2, label = 'Accelerated Proximal Gradient')\n",
    "ax.semilogy(t, np.array(obj_ADMM)-opt, 'r', linewidth = 2, label = 'ADMM')\n",
    "\n",
    "ax.legend(prop={'size':12})\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Objective error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
